>>>markdown


In this project i would like to learn:
- how to use Malloy language on transactional data
- how the modelling process looks like
- how to use Malloy notebooks
- how Malloy is different from DAX (language used in Power Bi)


To achieve this i've chosen Northwind dataset and would like to answer all the questions from Curbal's '25 Days of DAX Fridays! Challenge â€“ Ed1: NorthWind Company' (https://curbal.com/25-days-of-dax-fridays-challenge-ed1-northwind-company). This challenge is a great way to train DAX skills and i highly recommend it to anyone working with Power Bi. 

The source to learn about Malloy language:
https://malloydata.github.io/documentation/

I highly advise to visit Slack channel where Malloy's creators quickly answer all questions.

>>>markdown


I've decided to use csv files from this source:
https://github.com/graphql-compose/graphql-compose-examples/tree/master/examples/northwind/data/csv
When using csv or parquet files DuckDb is used as underlying engine. 
There are also options to connect to BigQuery or Postgres.

Dataset consists of 11 tables (2 tables are missing: customer_customer_demo and customer_demographics).

Schema: (from https://github.com/yugabyte/yugabyte-db/wiki/Northwind-Sample-Database)

![Alt text](schema.png)

To start using Malloy i need to first create model. That includes defining data sources, specifying joins, creating dimensions and measures. Malloy is designed in a way to work swiftly with transactional data, so there is no need to create dimensional model (e.g. star schema). It is a different approach than Power Bi, where the best practice is to create star schema - this design is usually a guarantee of good performance and results in simplified DAX code. 

Nice pattern is to first define base sources from tables with all measure definitions and dimensions. This would allow an analyst to get most of interesting numbers from those tables.
Next step would to be to extend base sources by joining them together and calculating cross-table metrics.

>>>markdown

Let's try to answer first question. I need to use products tables here, so i am going to define 'Products' source and extend it with simple measures.
>>>malloy
source: Products is duckdb.table('../northwind_test/parquet/products.parquet') 
    extend {
        
        measure: product_count is count()
        measure: total_price is unitPrice.sum()
        measure: avg_price is round(unitPrice.avg(),2)
        measure: all_avg_price is all(avg(unitPrice))
        measure: all_max_price is all(max(unitPrice))
      
        
}
>>>markdown

Question 1: How many current products cost less then 20$?
>>>malloy

run: Products -> {
    aggregate: product_count --measure
    where: discontinued = 0 and
    unitPrice < 20
}
>>>markdown

Question 2: Which product is the most expensive? Success at third try. Currently correlated subqueries are not available in Malloy so i added a column with ALL (like calculated column in PBI) and then filtered by the value in this column:
>>>malloy



-- First attempt returned both name and price:
run: Products -> {
    project: productName, unitPrice
    order_by: unitPrice desc
    limit: 1
}

-- Second version to have the same result, this time instead of run i can define a query:
query: top_customer is Products -> {
  group_by: productName
  aggregate: total_price --measure
  limit: 1
}

--Third version using pipeline, this time only name is returned.
run:  Products -> {
   group_by: productName, unitPrice    
   aggregate: all_max_price  --measure
    
} -> {
    project: productName
    where: unitPrice = all_max_price  
}


>>>markdown

Question 3: What is the average unit price for our products?
>>>malloy


run: Products -> {
    aggregate: avg_price  --measure
}
>>>markdown

Question 4: How many products are above the average unit price? Here using pipeline again:
>>>malloy

-- First version = pipeline
run:  Products -> {
   group_by: productName, unitPrice    
   aggregate: all_avg_price  --measure: all(avg(unitPrice))
    
} -> {
    where: unitPrice > all_avg_price
    aggregate: result is count()
}


        
>>>markdown

As an excercise this can be also achieved by writing query and then running our source with this query joined in 'extend':
>>>malloy


-- query
query: temp_table is  Products -> {
    group_by: productID   
    aggregate: all_avg_price --measure: all(avg(unitPrice))
}


-- source with join
run: Products 
    extend {
        join_one: temp_table is from(->temp_table) on productID = temp_table.productID
        
    } -> {
    where: unitPrice > temp_table.all_avg_price
    aggregate: result is count()
    }
>>>markdown

Question 5: How many products cost between $15 and $25? (inclusive)
>>>malloy

run: Products -> {
    where: unitPrice >= 15 and unitPrice <= 25
    aggregate: product_count
}
>>>markdown

Question 6: What is the average number of products (not qty) per order? - we need to calculate average order size

First i will define new source 'OrdersDetails':
>>>malloy

source: OrderDetails is duckdb.table('../northwind_test/parquet/order_details.parquet')
    extend {
         dimension: revenue is unitPrice * quantity
         
        -- join_one: Orders on Orders.orderID = orderID
    }
>>>markdown

Now i will write a pipeline query to calculate the result:
>>>malloy

run: OrderDetails -> {
    group_by: orderID
    aggregate: product_count is count()
       
} -> {
    aggregate: result is avg(product_count)
}

>>>markdown

Question 7: What is the order value in $ of open orders? (not shipped)

For the first time we will need to use two tables to calculate the result: orders and order_detail. A nice thing about Malloy is that it's very felxible. I can start from both sources and get correct result (of course i need those sources to be defined!)


I will start with creating Orders source and extending it with a join. I am using join_many as for one row in Orders source i have multiple rows in OrderDetails source. I am adding final query that returns correct result:
>>>malloy
source: Orders is duckdb.table('../northwind_test/parquet/orders.parquet') 

    extend {
        -- i can define primary_key
        primary_key: orderID
        -- here i define join
        join_many: OrderDetails on OrderDetails.orderID = orderID
    
        -- and here is final query giving me the correct result:
        query: q7_result is {
            where: shippedDate = null
            aggregate: orders_value is OrderDetails.revenue.sum() --as dimension 'revenue' is foreign i need to put sum() at the end
        }
    }
>>>markdown
If i start from OrderDetails i have two options:
1. using pipeline to extend this source with join to Orders and in next step calculate the answer (so it is 'run: source extend {}' option )
2. defining new source from OrderDetails but give it different name and extend it ('source: OrderDetails_ext is OrderDetails extend {}' option)


 I am using join_one as for each record in OrderDetails i have only one matching record in Orders source. Important thing is that i move back to my OrderDetails source and added dimension 'revenue' in extend clause. It is like a calculated column in Power Bi.
>>>malloy
run: OrderDetails
    extend {
         join_one: Orders on Orders.orderID = orderID
    } -> {
        where: Orders.shippedDate = null
        aggregate: test is sum(revenue) --as my 'revenue' dimension is local i can use sum(revenue)
    }
>>>markdown
Question 8: How many orders are single item (only one product ordered)?
>>>malloy
query: single_item_order is OrderDetails -> {
    group_by: orderID 
    aggregate: item_count is all(count(), orderID) --i am using grouping_dimension here
} -> {
    where: item_count = 1
    aggregate: result is count()
}
>>>markdown
Question 9: Avg sales per transaction for "Romero y Tobillo"